import React from "react";
// import Slideshow from "../../components/Slideshow/Slideshow";
import VideoPlayer from "../../components/VideoPlayer/VideoPlayer";
import VideoPlayerContainer from "../../components/VideoPlayerContainer/VideoPlayerContainer";
import ImageWithCaption from "../../components/ImageWithCaption/ImageWithCaption";
import PCMVContainer from "../../components/PCMVContainer/PCMVContainer";
import styles from "../../Common.module.css";
// import { useNavigate } from "react-router-dom";

// const images = [
//   "/hotformerloc/assets/slides/karawatha_aerial_1.png",
//   "/hotformerloc/assets/slides/karawatha_ground_1.png",
//   "/hotformerloc/assets/slides/karawatha_aerial_2.png",
//   "/hotformerloc/assets/slides/karawatha_ground_2.png",
//   "/hotformerloc/assets/slides/karawatha_aerial_3.png",
//   "/hotformerloc/assets/slides/karawatha_ground_3.png",

//   "/hotformerloc/assets/slides/venman_aerial_1.png",
//   "/hotformerloc/assets/slides/venman_ground_1.png",
//   "/hotformerloc/assets/slides/venman_aerial_2.png",
//   "/hotformerloc/assets/slides/venman_ground_2.png",
//   "/hotformerloc/assets/slides/venman_aerial_3.png",
//   "/hotformerloc/assets/slides/venman_ground_3.png",
// ];

const Dataset: React.FC = () => {
  return (
    <div className={styles.container}>
      <Header />
      <main className={styles.main}>
        {/* <SlideshowSection /> */}
        <VideoSection/>
        <Links />
        <OverviewSection />
        <VisualisationSection />
        <MethodologySection />
        <BenchmarkingSection />
      </main>
    </div>
  );
};

const Header: React.FC = () => (
  <header className={styles.header}>
    <h1 className={styles.title}>CS-Wild-Places: A Novel Benchmark for Cross-Source Lidar Place Recognition in Forest Environments</h1>
  </header>
);

const Links: React.FC = () => {
  // const navigate = useNavigate();

  return (
    <div className={styles.buttonContainer}>
      {/* <a
        href="https://github.com/csiro-robotics/HOTFormerLoc"
        target="_blank"
        rel="noopener noreferrer"
        style={{
          textDecoration: 'none',
          color: '#FFF',
        }}
        className={styles.navButton}
      >
        GitHub
      </a> */}
      <a
        href="https://data.csiro.au/collection/csiro:64896"
        target="_blank"
        rel="noopener noreferrer"
        style={{
          textDecoration: 'none',
          color: '#FFF',
        }}
        className={styles.navButton}
      >
        Download
      </a>
      {/* <button className={styles.navButton} onClick={() => navigate("/download")}>
        Download
      </button> */}
    </div>
  );
};

// const SlideshowSection: React.FC = () => (
//   <section className={styles.section}>
//     <Slideshow images={images} />
//   </section>
// );

const VideoSection: React.FC = () => (
  <section className={styles.section}>
    <VideoPlayer src={"/hotformerloc/assets/visualisation/HOTFormerLoc_website_submap_preview.mp4"} />
  </section>
);

const OverviewSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="overview" className={styles.sectionHeading}>
      Overview
    </h2>
    <p className={styles.paragraph}>
      We present <b>CS-Wild-Places</b>, the first benchmark for <b>cross-source</b> ground-to-aerial lidar place
      recognition in forest environments. We build upon the 8 ground sequences introduced
      in the <a href="https://csiro-robotics.github.io/Wild-Places/">Wild-Places</a> dataset
      by capturing aerial lidar scans of Karawatha and Venman forests, forming our "Baseline"
      set. We further introduce ground and aerial lidar scans from two new forests: QCAT and
      Samford Ecological Research Facility, forming our "Unseen" testing set. 
      Our dataset features:
    </p>
    <ul className={styles.list}>
      <li>
        High-resolution lidar scans captured from ground and aerial perspectives
        over 10 months in four unique forest environments in Brisbane,
        Australia.
      </li>
      <li>
        370 hectares of aerial lidar coverage and 3.5 km of ground traversals, 
        supplementing the 32.9 km of ground traversals in Wild-Places with up to
        3 years between ground and aerial scans.
      </li>
      <li>
        Accurate 6-DoF ground truth pose generated by Wildcat SLAM,
        geo-registered with RTK GNSS.
      </li>
      <li>
        Over 72k submaps for training and over 18k submaps for evaluation, split
        into Baseline and Unseen sets to test downstream generalisation.
      </li>
      <li>
        Challenging representational gaps such as variable point density and
        significant occlusions between viewpoints.
      </li>
    </ul>
    <ImageWithCaption
      src="/hotformerloc/assets/dataset/dataset_ground_aerial.png"
      alt="Ground and Aerial"
      caption="(Top row) Visualisation of the aerial global map of all four forests in CS-Wild-Places, with corresponding ground trajectories overlayed.
        (Bottom row) Submaps representing a scene from the ground and aerial perspective in each forest. The dense forest canopy creates major occlusions between viewpoints."
    />
    <ImageWithCaption
      src="/hotformerloc/assets/dataset/dataset_comparison.png"
      alt="Comparing Datasets"
      caption="Comparison of CS-Wild-Places with popular LPR benchmarks"
    />
    <p className={styles.paragraph}>
      We release the data in two main configurations: raw (submaps randomly
      downsampled to 500k points max), and post-processed (submaps voxel-downsampled
      with 0.8m voxels, ground points removed, with and without normalisation).
    </p>
  </section>
);

const MethodologySection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="methodology" className={styles.sectionHeading}>
      Methodology
    </h2>
    <h3 id="data-collection" className={styles.subHeading}>
      Data Collection
    </h3>
    <p className={styles.paragraph}>
      {/* Ground data collection uses a handheld perception pack with a VLP-16 lidar sensor, collected on foot. 
      We employed a SLAM system integrating GPS, IMU, and lidar to generate globally consistent maps and 
      near-ground truth trajectories. To capture aerial maps with varying
      characteristics, we collected data in two settings. For the main dataset (“Baseline”),
      we used a DJI M300 RTK quadcopter with a VLP-32C lidar sensor. For a smaller,
      test-only dataset (“Unseen”), we used an Acecore NOA hexacopter equipped
      with a RIEGL VUX-120 pushbroom lidar. Both drones flew in a lawnmower pattern
      over forested areas at a consistent height of 50 - 100m above the canopy. GPS
      RTK is used for all ground and aerial scans to ensure geo-registration in
      UTM coordinates. To refine alignment between overlapping ground and aerial
      areas, we apply iterative closest point (ICP) until the RMSE between
      correspondences is ≤ 0.5m. */}
      Ground data collection uses a handheld perception pack with spinning VLP-16
      lidar sensor. We capture 2 sequences on foot in QCAT and Samford, 
      supplementing the 8 Wild-Places sequences for a total of 36.4 km traversal
      over 13.8 km of trails. To generate globally consistent maps and near-ground
      truth trajectories, we use Wildcat SLAM, integrating GPS, IMU, and lidar.
      
      For aerial data collection, we deployed two drone configurations. For
      Karawatha, Venman, and QCAT, we used a DJI M300 quadcopter with a VLP-32C
      lidar sensor. For Samford, we used an Acecore NOA hexacopter equipped
      with a RIEGL VUX-120 pushbroom lidar. Both drones flew in a lawnmower
      pattern over forested areas at a consistent height of 50-100m above the canopy.
      GPS RTK is used for all aerial scans to ensure precise geo-registration in
      UTM coordinates. We align overlapping ground and aerial areas using
      iterative closest point until the RMSE between correspondences is ≤ 0.5m.
    </p>
    <h3 id="submap-generation" className={styles.subHeading}>
      Submap Generation
    </h3>
    <p className={styles.paragraph}>
      {/* We follow two protocols to generate lidar submaps for training and testing.
      Ground submaps are sampled every two seconds along each trajectory,
      aggregating all points captured within one second of the corresponding
      timestamp, within a 30m horizontal radius. Points are stored in the local
      coordinate system of the sensor, along with the 6-DoF pose in UTM coordinates.
      Aerial submaps are uniformly sampled from a 10m
      spaced grid spanning the aerial map at the average flight height, and
      stored in a pesudo-local coordinate system from each sampled location. To
      create a realistic scenario, grid borders are set to sample a much larger area
      than is covered by the ground traversals. For consistency
      with ground submaps, we limit submaps to a 30m horizontal radius.
      This produces a set of overlapping aerial patches that
      form a comprehensive database covering each forest.
      We further post-process submaps, removing all points
      situated on the ground plane using a Cloth Simulation Filter
      (CSF). For training LPR methods, we elect to voxel
      downsample submaps with voxel size 0.8m, generating
      submaps with an average of 28K points. */}
      We follow two protocols to generate lidar submaps suitable for LPR. Ground
      submaps are sampled at 0.5 Hz along each trajectory, aggregating all points
      captured within a two second sliding window of the corresponding timestamp,
      within a 30m horizontal radius. Points are stored in the submap's local
      coordinates, along with the 6-DoF pose in UTM coordinates.

      Aerial submaps are uniformly sampled from a 10m spaced grid spanning
      the aerial map. To create a realistic scenario, grid borders are set to
      sample a much larger area than is covered by the ground traversals. For
      consistency with ground submaps, we limit submaps to a 30m horizontal
      radius. This produces a set of overlapping aerial patches that form a
      comprehensive database covering each forest. 

      We further post-process submaps, removing all points situated on the
      ground plane using a Cloth Simulation Filter (CSF). To save computation,
      we voxel downsample submaps with voxel size of 0.8m, generating submaps
      with an average of 28k points.
    </p>
    <h3 id="train-eval" className={styles.subHeading}>
      Training and Evaluation Splits
    </h3>
    <p className={styles.paragraph}>
      {/* We train LPR methods using submaps from Karawatha and Venman, and
      following Wild-Places, we withhold a disjoint set of submaps to form the
      Baseline evaluation set. To prevent information leakage between the training
      and evaluation sets, we exclude any submaps from training
      that have overlap with the evaluation queries. For optimising
      triplet-based losses, we construct training tuples with a
      15m positive threshold, and 60m negative threshold.
      During evaluation, we consider the withheld baseline
      ground submaps as queries, and use all aerial submaps as a
      per-forest database. We also provide the Unseen set to test
      generalisation on unseen environments, using all submaps
      from QCAT and Samford forests to form the ground queries and a
      per-forest aerial database. We consider a true positive retrieval
      threshold of 30m during evaluation. */}
      We train LPR methods using submaps from the Baseline set, and withhold a
      disjoint set of submaps for evaluation, following the test regions of
      Wild-Places. To prevent information leakage between training and evaluation,
      we exclude any submaps from training that overlap the evaluation queries.
      For optimising triplet-based losses, we construct training tuples with a
      15m positive threshold, and 60m negative threshold.
      During evaluation, we use the withheld Baseline ground submaps as queries,
      and all aerial submaps as a per-forest database.
      We test generalisation to new environments on our Unseen test set, using all
      submaps in the set to form the ground queries and per-forest aerial database.
      We consider a true positive retrieval threshold of 30m during evaluation.
    </p>
  </section>
);

const BenchmarkingSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="benchmarking" className={styles.sectionHeading}>
      Benchmarking
    </h2>
    <p>
      We benchmark several SOTA LPR methods on our dataset, including our novel HOTFormerLoc,
      to demonstrate the challenges of cross-source re-localisation within dense forests. 
      We report Average Recall@N (AR@N), including variants such as AR@1 and AR@1%, where 1%
      is 1% of the number of submaps in the database.
    </p>
    <div className={styles.imageGrid}>
      <ImageWithCaption
        src="/hotformerloc/assets/dataset/experiments_benchmarking.png"
        alt="Recall@N curves on CS-Wild-Places."
        caption="Recall@N curves of SOTA LPR methods on CS-Wild-Places Baseline and Unseen splits."
      />
    </div>
    <div className={styles.imageGrid}>
      <ImageWithCaption
        src="/hotformerloc/assets/dataset/dataset_cswp_baseline.png"
        alt="SOTA results on CS-Wild-Places Baseline evaluation."
        caption="Comparison of SOTA on CS-Wild-Places baseline evaluation set."
      />
    </div>
    <div className={styles.imageGrid}>
      <ImageWithCaption
        src="/hotformerloc/assets/dataset/dataset_cswp_unseen.png"
        alt="SOTA results on CS-Wild-Places Unseen evaluation."
        caption="Comparison of SOTA on CS-Wild-Places Unseen evaluation set."
      />
    </div>
  </section>
);

const VisualisationSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="visualisation" className={styles.sectionHeading}>
      Visualisation
    </h2>

    <div>
      <h3 id="submap" className={styles.subHeading}>
        Interactive Submap Visualisation
      </h3>
      <div>
        <PCMVContainer
          title1="Aerial View"
          title2="Ground View"
          isSingleViewer={false}
        />
      </div>
    </div>

    <div>
      <h3 id="aerial-map" className={styles.subHeading}>
        Aerial Map Visualisation
      </h3>
      <div className={styles.imageGrid}>
        <figure>
          <VideoPlayerContainer
            src_1_title="Karawatha"
            src_1_path="/hotformerloc/assets/visualisation/karawatha_aerial_vid.mp4"
            src_2_title="Venman"
            src_2_path="/hotformerloc/assets/visualisation/venman_aerial_vid.mp4"
          />
          <VideoPlayerContainer
            src_1_title="QCAT"
            src_1_path="/hotformerloc/assets/visualisation/qcat_aerial_overview_vid.mp4"
            src_2_title="Samford"
            src_2_path="/hotformerloc/assets/visualisation/samford_aerial_overview_vid.mp4"
          />
          <figcaption></figcaption>
        </figure>
      </div>

      <h3 id="ground-aerial-map" className={styles.subHeading}>
        Ground vs Aerial Map Comparison
      </h3>
      <div className={styles.imageGrid}>
        <ImageWithCaption
          src="/hotformerloc/assets/visualisation/ground_aerial_1.png"
          alt="Ground Vs Aerial Lidar Visualisation"
          caption="Ground (top) vs Aerial (bottom) lidar scans for a section of Karawatha (note that a vertical offset is added manually for visualisation purposes)."
        />
      </div>
    </div>
  </section>
);

export default Dataset;
