import React from "react";
import Slideshow from "../../components/Slideshow/Slideshow";
import VideoPlayerContainer from "../../components/VideoPlayerContainer/VideoPlayerContainer";
import ImageWithCaption from "../../components/ImageWithCaption/ImageWithCaption";
import PCMVContainer from "../../components/PCMVContainer/PCMVContainer";
import styles from "../../Common.module.css";

const images = [
  "/hotformerloc/assets/slides/karawatha_aerial_1.png",
  "/hotformerloc/assets/slides/karawatha_ground_1.png",
  "/hotformerloc/assets/slides/karawatha_aerial_2.png",
  "/hotformerloc/assets/slides/karawatha_ground_2.png",
  "/hotformerloc/assets/slides/karawatha_aerial_3.png",
  "/hotformerloc/assets/slides/karawatha_ground_3.png",

  "/hotformerloc/assets/slides/venman_aerial_1.png",
  "/hotformerloc/assets/slides/venman_ground_1.png",
  "/hotformerloc/assets/slides/venman_aerial_2.png",
  "/hotformerloc/assets/slides/venman_ground_2.png",
  "/hotformerloc/assets/slides/venman_aerial_3.png",
  "/hotformerloc/assets/slides/venman_ground_3.png",
];

const Dataset: React.FC = () => {
  return (
    <div className={styles.container}>
      <Header />
      <main className={styles.main}>
        <SlideshowSection />
        <OverviewSection />
        <VisualisationSection />
        <MethodologySection />
        <BenchmarkingSection />
      </main>
    </div>
  );
};

const Header: React.FC = () => (
  <header className={styles.header}>
    <h1 className={styles.title}>CS-Wild-Places: A Novel Benchmark for Cross-Source Lidar Place Recognition in Forest Environments</h1>
  </header>
);

const SlideshowSection: React.FC = () => (
  <section className={styles.section}>
    <Slideshow images={images} />
  </section>
);

const OverviewSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="overview" className={styles.sectionHeading}>
      Overview
    </h2>
    <p className={styles.paragraph}>
      We present <b>CS-Wild-Places</b>, the first benchmark for <b>cross-source</b> ground-to-aerial lidar place
      recognition in forest environments. We build upon the ground lidar scans provided
      in the <a href="https://csiro-robotics.github.io/Wild-Places/">Wild-Places</a> dataset
      by introducing lidar scans captured from an aerial viewpoint, as 
      well as ground and aerial scans captured in two new forest environments.
      Our dataset features:
    </p>
    <ul className={styles.list}>
      <li>
        High-resolution lidar scans captured from ground and aerial perspectives
        over a three-year span in four unique forest environments in Brisbane,
        Australia
      </li>
      <li>
        Accurate 6-DoF ground truth pose generated by Wildcat SLAM,
        geo-registered with RTK GNSS
      </li>
      <li>
        Over 72K submaps for training and over 18K submaps for evaluation, split
        into Baseline and Unseen sets to test downstream generalisation
      </li>
      <li>
        Challenging representational gaps such as variable point density and
        significant occlusions between viewpoints
      </li>
    </ul>
    <ImageWithCaption
      src="/hotformerloc/assets/dataset/dataset_ground_aerial.png"
      alt="Ground and Aerial"
      caption="(Top row) Visualisation of the aerial global map of all four forests in CS-Wild-Places, with corresponding ground trajectories overlayed.
        (Bottom row) Submaps representing a scene from the ground and aerial perspective in each forest. The dense forest canopy creates major occlusions between viewpoints."
    />
  </section>
);

const MethodologySection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="methodology" className={styles.sectionHeading}>
      Methodology
    </h2>
    <h3 id="data-collection" className={styles.subHeading}>
      Data Collection
    </h3>
    <p className={styles.paragraph}>
      Ground data collection uses a handheld perception pack with a VLP-16 lidar sensor, collected on foot. 
      We employed a SLAM system integrating GPS, IMU, and lidar to generate globally consistent maps and 
      near-ground truth trajectories. To capture aerial maps with varying
      characteristics, we collected data in two settings. For the main dataset (“Baseline”),
      we used a DJI M300 RTK quadcopter with a VLP-32C lidar sensor. For a smaller,
      test-only dataset (“Unseen”), we used an Acecore NOA hexacopter equipped
      with a RIEGL VUX-120 pushbroom lidar. Both drones flew in a lawnmower pattern
      over forested areas at a consistent height of 50 - 100m above the canopy. GPS
      RTK is used for all ground and aerial scans to ensure geo-registration in
      UTM coordinates. To refine alignment between overlapping ground and aerial
      areas, we apply iterative closest point (ICP) until the RMSE between
      correspondences is ≤ 0.5m.
    </p>
    <h3 id="submap-generation" className={styles.subHeading}>
      Submap Generation
    </h3>
    <p className={styles.paragraph}>
      We follow two protocols to generate lidar submaps for training and testing.
      Ground submaps are sampled every two seconds along each trajectory,
      aggregating all points captured within one second of the corresponding
      timestamp, within a 30m horizontal radius. Points are stored in the local
      coordinate system of the sensor, along with the 6-DoF pose in UTM coordinates.
      Aerial submaps are uniformly sampled from a 10m
      spaced grid spanning the aerial map at the average flight height, and
      stored in a pesudo-local coordinate system from each sampled location. To
      create a realistic scenario, grid borders are set to sample a much larger area
      than is covered by the ground traversals. For consistency
      with ground submaps, we limit submaps to a 30m horizontal radius.
      This produces a set of overlapping aerial patches that
      form a comprehensive database covering each forest.
      We further post-process submaps, removing all points
      situated on the ground plane using a Cloth Simulation Filter
      (CSF). For training LPR methods, we elect to voxel
      downsample submaps with voxel size 0.8m, generating
      submaps with an average of 28K points.
    </p>
    <h3 id="train-eval" className={styles.subHeading}>
      Training and Evaluation Splits
    </h3>
    <p className={styles.paragraph}>
      We train LPR methods using submaps from Karawatha and Venman, and
      following Wild-Places, we withhold a disjoint set of submaps to form the
      Baseline evaluation set. To prevent information leakage between the training
      and evaluation sets, we exclude any submaps from training
      that have overlap with the evaluation queries. For optimising
      triplet-based losses, we construct training tuples with a
      15m positive threshold, and 60m negative threshold.
      During evaluation, we consider the withheld baseline
      ground submaps as queries, and use all aerial submaps as a
      per-forest database. We also provide the Unseen set to test
      generalisation on unseen environments, using all submaps
      from QCAT and Samford forests to form the ground queries and a
      per-forest aerial database. We consider a true positive retrieval
      threshold of 30m during evaluation.
    </p>
    <div className={styles.imageGrid}>
      <ImageWithCaption
        src="/hotformerloc/assets/dataset/dataset_comparison.png"
        alt="Comparing Datasets"
        caption="Comparison of CS-Wild-Places with popular LPR benchmarks"
      />
    </div>
  </section>
);

const BenchmarkingSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="benchmarking" className={styles.sectionHeading}>
      Benchmarking
    </h2>
    <p>
      We benchmark several SOTA LPR methods on our dataset, including our novel HOTFormerLoc,
      to demonstrate the challenges of cross-source re-localisation within dense forests. 
    </p>
    <div className={styles.imageGrid}>
      <ImageWithCaption
        src="/hotformerloc/assets/dataset/experiments_benchmarking.png"
        alt="Benchmarking Results"
        caption="Recall@N curves of four SOTA LPR methods on CS-Wild-Places Baseline and Unseen splits"
      />
    </div>
  </section>
);

const VisualisationSection: React.FC = () => (
  <section className={styles.section}>
    <h2 id="visualisation" className={styles.sectionHeading}>
      Visualisation
    </h2>

    <div>
      <h3 id="submap" className={styles.subHeading}>
        Interactive Submap Visualisation
      </h3>
      <div>
        <PCMVContainer
          title1="Aerial View"
          title2="Ground View"
          isSingleViewer={false}
        />
      </div>
    </div>

    <div>
      <h3 id="aerial-map" className={styles.subHeading}>
        Aerial Map Visualisation
      </h3>
      <div className={styles.imageGrid}>
        <figure>
          <VideoPlayerContainer
            src_1_title="Karawatha"
            src_1_path="/hotformerloc/assets/visualisation/karawatha_aerial_vid.mp4"
            src_2_title="Venman"
            src_2_path="/hotformerloc/assets/visualisation/venman_aerial_vid.mp4"
          />
          <figcaption>Global maps produced with aerial lidar scans from Karawatha and Venman.</figcaption>
        </figure>
      </div>

      <h3 id="ground-aerial-map" className={styles.subHeading}>
        Ground vs Aerial Map Comparison
      </h3>
      <div className={styles.imageGrid}>
        <ImageWithCaption
          src="/hotformerloc/assets/visualisation/ground_aerial_1.png"
          alt="Ground Vs Aerial Lidar Visualisation"
          caption="Ground (top) vs Aerial (bottom) lidar scans for a section of Karawatha (note that a vertical offset is added manually for visualisation purposes)"
        />
      </div>
    </div>
  </section>
);

export default Dataset;
